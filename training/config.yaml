base: unsloth/gpt-oss-20b-unsloth-bnb-4bit
max_len: 2048
lora:
  r: 8
  alpha: 16
  dropout: 0.1
  target_modules: [q_proj, k_proj, v_proj, o_proj]
layers: [19,20,21,22,23]         # freeze adapters outside these layers
optim:
  lr: 5e-5
  warmup_ratio: 0.05
  grad_clip: 0.3
trainer:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  max_steps: 3000
  logging_steps: 25
  save_steps: 200
  save_total_limit: 5
  fp16: false
  bf16: false