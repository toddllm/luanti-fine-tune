# üéØ Luanti Capability ‚Äî GPT-OSS 20B (QLoRA on RTX 3090)

**üéâ CHECKPOINT 500 ACHIEVED - GATES D+E EXECUTING AUTOMATICALLY**

A reproducible pipeline to fine-tune `unsloth/gpt-oss-20b-unsloth-bnb-4bit` into a **Luanti mod expert** using QLoRA. Includes baseline eval (Gate B), training (Gate C), adapter testing (Gate D), and comparison (Gate E), with tmux-based monitoring and promotion tools.

## üî• **Current Status**
- **Training**: Step 516/1500 (34.4% complete)
- **Loss**: 0.16 (93% reduction from 2.209!)
- **Checkpoint**: 500 achieved and saved
- **Gate D**: Auto-triggered adapter testing in progress
- **Gate E**: Baseline comparison executing
- **Target**: +15pp improvement (28.33% ‚Üí 43.33%+ pass@5)

---

## ‚úÖ **Known-Good Environment (LOCKED)**
- Python: 3.12 (conda env **gptoss**)
- PyTorch: **2.5.0+cu124**
- CUDA Toolkit: **12.4**
- Unsloth: **2025.8.7**
- Transformers: **4.56.0.dev0**
- xFormers: **0.0.28.post2**
- GPU: RTX 3090 (24GB)

> Keep `TORCHDYNAMO_DISABLE=1` and **do not** set `HF_HUB_OFFLINE=1`. Cache is local via `HF_HOME`.

---

## üìã **Project Structure**
```
luanti_capability/
‚îú‚îÄ‚îÄ env/                    # Environment helpers and version locks
‚îú‚îÄ‚îÄ data/                   # Training (600) and evaluation (60) datasets
‚îú‚îÄ‚îÄ eval/                   # Evaluation scripts and results
‚îú‚îÄ‚îÄ prompts/                # IIR template and formatter
‚îú‚îÄ‚îÄ training/               # Training configurations and scripts
‚îú‚îÄ‚îÄ outputs_luanti_safe/    # Training outputs and checkpoints
‚îú‚îÄ‚îÄ scripts/                # Monitoring, health checks, automation
‚îú‚îÄ‚îÄ gen.py                  # Direct generation with best adapter
‚îî‚îÄ‚îÄ run_gateb_train.sh      # Gate B parity training launcher
```

---

## üöÄ **Usage**

### **Environment Setup (Always Required)**
```bash
source "$HOME/miniconda3/etc/profile.d/conda.sh"
conda activate gptoss
export HF_HOME="$PWD/.cache/hf"; export TRANSFORMERS_CACHE="$HF_HOME"
export TORCHDYNAMO_DISABLE=1 PYTHONUNBUFFERED=1 TOKENIZERS_PARALLELISM=false
```

### **Training (Gate C)**
```bash
bash ./run_gateb_train.sh
# Monitor: tail -f outputs_luanti_safe/training.log
```

### **Auto Gate D/E (Checkpoint Detection)**
```bash
tmux new -ds luanti_auto 'bash -lc "
source $HOME/miniconda3/etc/profile.d/conda.sh && conda activate gptoss;
TORCHDYNAMO_DISABLE=1 PYTHONUNBUFFERED=1 \
/home/tdeshane/miniconda3/envs/gptoss/bin/python -u scripts/auto_gateDE.py \
| tee -a eval/results/auto_gateDE.log"'
```

### **Manual Gate D (Adapter Testing)**
```bash
TORCHDYNAMO_DISABLE=1 /home/tdeshane/miniconda3/envs/gptoss/bin/python -u -m eval.test_adapter \
  --base unsloth/gpt-oss-20b-unsloth-bnb-4bit \
  --adapters_dir outputs_luanti_safe \
  --eval data/eval/luanti_eval.jsonl \
  --template prompts/iir_template.txt \
  --k 5 --temperature 0.2 --top_p 0.9 --max_new_tokens 300 \
  --scales 0.25 0.5 1.0 --seed 3407 \
  --out_dir eval/results/ | tee eval/results/test_adapter.log
```

### **Gate E (Comparison)**
```bash
/home/tdeshane/miniconda3/envs/gptoss/bin/python -u -m eval.compare \
  --baseline eval/results/baseline.json \
  --results_dir eval/results/ | tee eval/results/deltas.txt
```

### **Promotion and Generation**
```bash
# Promote best adapter
python -u scripts/promote_best.py

# Package release
bash scripts/package_release.sh

# Direct generation
python -u gen.py "Create a Luanti node that emits light_source=14 and drops itself when dug."
```

---

## üìä **Results**

### **Baseline (Gate B)**
- **Overall pass@5**: 28.33%
- **scaffold**: 5.00% (node registration)  
- **repair**: 0.00% (code fixing)
- **doc**: 80.00% (API usage)

### **Training Progress (Gate C)**
- **Loss improvement**: 2.209 ‚Üí 0.16 (93% reduction)
- **Steps completed**: 516/1500 (34.4%)
- **Training quality**: Exceptional convergence
- **GPU utilization**: 13.4GB, 81% utilization

### **Success Criteria**
- **Target**: +15pp improvement in pass@5 (43.33%+ overall)
- **Focus**: Improve scaffold and repair tasks
- **Quality**: Maintain doc task performance

---

## üîß **Technical Details**

### **QLoRA Configuration**
- **Base model**: unsloth/gpt-oss-20b-unsloth-bnb-4bit
- **LoRA rank**: 8 (conservative for MoE stability)
- **Target modules**: Attention-only (q_proj, k_proj, v_proj, o_proj)
- **MoE safety**: No expert/router modifications
- **Batch size**: 16 effective (2 √ó 8 gradient accumulation)

### **Safety Measures**
- **dtype alignment**: LoRA matrices aligned to torch.bfloat16
- **Compiled cache**: Protected with accelerator_scaler injection
- **TorchDynamo**: Disabled to prevent compilation errors
- **Environment**: Locked to proven working versions

---

## üîç **Monitoring**

### **Health Checks**
```bash
bash scripts/health.sh              # Complete system status
bash scripts/watchdog.sh            # NaN/stall detection
tail -f outputs_luanti_safe/training.log  # Live training progress
```

### **Live Monitoring**
```bash
# GPU + log watch
tmux attach -t luanti_watch

# Auto-evaluation status
tail -f eval/results/auto_gateDE.log
```

---

## üõ°Ô∏è **Troubleshooting**

### **Common Issues**
- **Model loading hangs**: Verify `gptoss` environment, unset `HF_HUB_OFFLINE`
- **Accelerator_scaler error**: Clear compiled cache, restart with `TORCHDYNAMO_DISABLE=1`
- **Import warnings**: Ensure unsloth imported before transformers
- **Environment drift**: Always source working environment setup

### **Recovery Procedures**
- **Training restart**: `bash run_gateb_train.sh` (auto-resumes from last checkpoint)
- **Manual evaluation**: Use individual Gate D/E commands above
- **Environment reset**: `source env/enter_gptoss_tmux.sh`

---

## üìÑ **License**
Apache 2.0 - Safe for commercial and research use.

---

## üôè **Acknowledgments**
- **[Unsloth](https://unsloth.ai)** for 20B parameter fine-tuning on consumer hardware
- **GPT-OSS** for the excellent 20B MoE base model
- **Luanti Community** for the rich modding ecosystem

---

**Last Updated**: September 2, 2025  
**Status**: Gate C complete, Gates D+E executing automatically  
**Next**: Results verification and best adapter promotion